{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLr5AgSzZddO"
      },
      "outputs": [],
      "source": [
        "## **In This Notebook, I have built a Resume Parser Application using Llama2 model and langchain framework**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq torch==2.0.0 --progress-bar off\n",
        "!pip install -qqq transformers==4.31.0 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.266 --progress-bar off\n",
        "!pip install -qqq openai==0.27.4 --progress-bar off\n",
        "!pip install -Uqqq watermark==2.3.1 --progress-bar off\n",
        "!pip install -Uqqq chromadb==0.4.5 --progress-bar off\n",
        "!pip install -Uqqq tiktoken==0.3.3 --progress-bar off\n",
        "!pip install -Uqqq youtube-transcript-api==0.5.0 --progress-bar off\n",
        "!pip install -Uqqq pytube==12.1.3 --progress-bar off\n",
        "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off\n",
        "!pip install -qqq InstructorEmbedding==1.0.1  --progress-bar off\n",
        "!pip install -qqq xformers==0.0.20  --progress-bar off\n",
        "!pip install -Uqqq unstructured[local-inference]==0.5.12 --progress-bar off"
      ],
      "metadata": {
        "id": "FM8nnMrlay7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.4.1/auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "ovJ_UTDxay98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl --progress-bar off"
      ],
      "metadata": {
        "id": "pV0z9PhqazAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import textwrap\n",
        "import torch\n",
        "import chromadb\n",
        "import langchain\n",
        "import openai\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import TextLoader, UnstructuredPDFLoader, YoutubeLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.llms import OpenAI, HuggingFacePipeline\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline, logging, TextStreamer\n",
        "from langchain.document_loaders.image import UnstructuredImageLoader"
      ],
      "metadata": {
        "id": "DwH1N4gWazCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_response(response: str):\n",
        "    print(\"\\n\".join(textwrap.wrap(response, width=100)))"
      ],
      "metadata": {
        "id": "7362ukftazFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_loader = UnstructuredPDFLoader(\"/kaggle/input/sampleresume/External_CV_V2.pdf\")"
      ],
      "metadata": {
        "id": "sw6TUawaazJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_pages = pdf_loader.load_and_split()"
      ],
      "metadata": {
        "id": "DUeDr6maa9KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=512)\n",
        "texts = text_splitter.split_documents(pdf_pages)\n",
        "len(texts)"
      ],
      "metadata": {
        "id": "qboxIGlYa9My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"hkunlp/instructor-large\"\n",
        "\n",
        "hf_embeddings = HuggingFaceInstructEmbeddings(\n",
        "    model_name = model_name) ## , model_kwargs = {'device': 'cuda'}"
      ],
      "metadata": {
        "id": "LcLNWFZHa9PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = Chroma.from_documents(texts, hf_embeddings)"
      ],
      "metadata": {
        "id": "N4v9xKaXa9Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
        "model_basename = \"gptq_model-4bit-128g\"\n",
        "\n",
        "use_triton = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
        "        model_basename=model_basename,\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=True,\n",
        "        device='cuda:0',\n",
        "        use_triton=use_triton,\n",
        "        quantize_config=None)"
      ],
      "metadata": {
        "id": "M6kvYW6Ya9UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamer = TextStreamer(tokenizer, skip_prompt = True, skip_special_tokens = True)\n",
        "text_pipeline = pipeline(task = 'text-generation', model = model, tokenizer = tokenizer, streamer = streamer)\n",
        "llm = HuggingFacePipeline(pipeline = text_pipeline)"
      ],
      "metadata": {
        "id": "8KKAN8y3a9Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(prompt, sys_prompt):\n",
        "    return f\"[INST] <<SYS>> {sys_prompt} <</SYS>> {prompt} [/INST]\""
      ],
      "metadata": {
        "id": "xq0mW-qOazMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys_prompt = \"Use following piece of context to answer the question in less than 20 words\"\n",
        "template = generate_prompt(\n",
        "    \"\"\"\n",
        "    {context}\n",
        "\n",
        "    Question : {question}\n",
        "    \"\"\"\n",
        "    , sys_prompt)\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "lqU5kB6BazPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
        "    return_source_documents = True,\n",
        "    chain_type_kwargs=chain_type_kwargs,\n",
        ")"
      ],
      "metadata": {
        "id": "uZ5fsda0bMVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\"what projects candidate worked on ?\")"
      ],
      "metadata": {
        "id": "nlgvmWOvbMYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\"where did candidate study?\")"
      ],
      "metadata": {
        "id": "e0IqhqM6bMai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WOyppaVrbMc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QZagshmnbMgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dImGc_UtbMlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fvn_IV6kbMoE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}